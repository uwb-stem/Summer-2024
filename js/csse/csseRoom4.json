{
    "csse": [
      
        {
           "time": "1:00 PM - 1:15 PM",
           "projectId": "csse-4-100",
           "title": "Interpreting ML Predictions in Alzheimer's Disease",
           "studentName": "Stanley Hsieh",
           "studentMajor": "CSSE",
           "projectType": "Faculty research",
           "facultyAdvisor": "Prof. Wooyoung Kim",
           "posterLink": "./posters/csse/hsiehstanley_4262399_122712236_Capstone_Poster.png",
           "abstract": "Alzheimer’s disease is a progressive disorder that slowly destroys important mental functions. This disease affects millions of people worldwide. There is no cure but there are treatments to help if it is diagnosed early enough. My motivation for this project stems from my desire to deepen my understanding of machine learning. After discussions with Professor Kim and the personal experience of my grandfather’s struggle with Alzheimer’s, the project was defined.\n\n The objective was to use machine learning models to not only accurately predict the potential of a patient developing or having Alzheimer’s but also being able to interpret the results, using techniques like anchors and counterfactuals. In this project I utilized four models: linear SVM, Logistic Regression, Decision Tree, and Random Forest. While other models were explored, these four provided the most accurate results. The results were then thrown into anchors and counterfactual methods to determine how and why each model came to each prediction. The data used was found on Kaggle, there was a lot of preprocessing that needed to be done before running the data. This included converting all the non-numeric data to fit the models’ requirements. Due to my small dataset, I retained the duplicate patient entries, to avoid significantly reducing the sample size.\n\n In conclusion, the findings revealed that the anchors were relatively straightforward, while counterfactuals were more challenging. Most of the anchors were consistent across all models, except the true positive anchors for random forest, they were much more specific, but with limited coverage. Leading to a deeper understanding of how and why the model makes its predictions. Counterfactuals were more challenging to distinguish because when comparing the original and counterfactual instance, there were many subtle changes, making it hard to discover which feature had the most impact.\n\n While the machine learning models demonstrated high accuracy, interpretability remains a significant challenge, limiting their use in the medical field. Further research with medical professionals should be done. Involving collaboration with medical professionals will enhance the integration of AI and ML into healthcare and developing a user-friendly interface."   

	},        
    {
        "time": "1:15 PM - 1:30 PM",
        "projectId": "csse-4-115",
        "title": "Analysis of PTSD Data Using Dynamic Time Warping and Clustering Techniques",
        "studentName": "Karen Reynaga",
        "studentMajor": "CSSE",
        "projectType": "Faculty research",
        "facultyAdvisor": "Prof. Wooyoung Kim",
        "posterLink": "./posters/csse/reynagagarciadealbakaren_4127918_122744178_Karen Reynaga CSS497 Poster.jpg",
        "abstract": "Sequence alignment is when two sequences are aligned to better identify the similarities between them. This has various applications from plagiarism detection to detecting similarities between species. In this project, it is used to compare PTSD scores from patient medical records. This helps us in sorting the medical records into different clusters based on the similarity of their PTSD scores over 10 years.\n\n We used three different metrics to cluster the medical records into three clusters. The metrics used were Dynamic Time Warping (DTW), Soft-DTW, and Euclidean. We used TSLearn to cluster the records using the three metrics and computed the silhouette score of the clusters using Scikit. The Silhouette score is used to evaluate the performance of the clustering. It produces a number between -1 and 1, where poor performance is -1 and almost perfect clustering is 1.\n\n We manually chose three seed patients to represent increasing, decreasing, and oscillating clusters. We then clustered the records using the three metrics mentioned above and evaluated their performance using the silhouette score. Soft- DTW scored 0.62, DTW scored 0.45, and Euclidean scored 0.41. Based on the silhouette score, Soft-DTW outperformed the other metrics when using real seed patients. We then synthesized seed patients which consisted of a dramatic increase, decrease, and oscillations. In this method, Soft-DTW scored 0.61, DTW scored 0.43, and Euclidean scored 0.40. Again, Soft-DTW outperformed the other metrics.\n\n At the end of this project, I concluded that the best metric for clustering the PTSD data used was Soft DTW due to it having the highest silhouette score when using both real and synthetic seed patients. Now that we know what the best metric for clustering is, we can utilize this information to analyze each cluster individually. Knowing the best metric for clustering this data also gives us an advantage if we want to cluster a similar dataset."   

     },
     {
        "time": "1:30 PM - 1:45 PM",
        "projectId": "csse-4-130",
        "title": "Software Engineering Internship at Datafi",
        "studentName": "Karan Hanswadkar",
        "studentMajor": "CSSE",
        "projectType": "Internship or Job opportunity",
        "facultyAdvisor": "Prof. Wooyoung Kim",
        "posterLink": "./posters/csse/hanswadkarkarankaivalya_4132430_122720219_CSS-Capstone-Poster-2024.jpg",
        "abstract": "Datafi is a software start-up company that offers a unified Artificial Intelligence (AI) data platform to address challenges related to data accessibility and decision-making.\n\n During my software internship experience at Datafi, I was able to work on several projects in both backend and frontend development and played a strong role in contributing to the platform’s enhancement and company’s success. At the start of my internship, my primary focus was to enhance Datafi’s AI model with a new summary feature. This task involved improving the platform’s NLP and NLQ capabilities. I achieved this by developing specific prompts for the Chat-GPT API and creating test scripts using Python and JSON files. The summary feature was designed to analyze the AI’s prompt responses and user inputs to generate detailed, personalized summaries. This significant improvement aimed to provide users with well-crafted and contextually relevant responses, thereby enhancing the overall user experience on the platform.\n\n For the front-end project, these consisted of tasks in the engineering backlog intended for improving the user experience and I was assigned these tasks based on my abilities and priority for the company. These frontend tasks consisted of learning and using Typescript language and the Quasar Framework to work on major projects such as redesigned modern navigation sidebar, and Data connector page and also minor projects such as creating legends for pie charts data and info button. \n\n The purpose of these tasks was to enhance Datafi’s platform by improving its NLP capabilities and user interface. The summary feature provides users with more accurate and personalized responses. For frontend improvements we were focused on creating a more efficient user experience, making the platform easier to navigate and interact with.\n\n The improvements made during my internship had a substantial impact on Datafi’s platform and also on my personal development. The AI summary feature provided users with more accurate and detailed summaries, thus following the company core values of making it easier for the average data worker to understand and interpret business data. The redesigned navigation sidebar and Data Connector page, along with the improvements in data visualization, enhanced the user experience for the platform and made navigation more efficient. These updates resulted in a better user engagement and high praise from potential customers. Additionally, attending daily meeting, working closely with professionals and various teams, and presenting these updates to the team helped me refine my communication skills."   
     }
     ,
     {
        "time": "1:45 PM - 2:00 PM",
        "projectId": "csse-4-145",
        "title": "Health Monitoring at Gekko Corporation",
        "studentName": "Zohair Ahmad",
        "studentMajor": "CSSE",
        "projectType": "Internship or Job opportunity",
        "facultyAdvisor": "Mr. Mark Kochanski",
        "posterLink": "./posters/csse/ahmadzohair_4284888_122745273_Poster.png",
        "abstract": "The rapid growth of wearable and IoT health devices, such as Apple Watches and glucose monitors, has resulted in a vast amount of data that is often underutilized in healthcare. Existing systems struggle to effectively integrate and analyze this data, leading to missed opportunities for real-time health monitoring and personalized care.\n\n My capstone project aims to solve this issue by leveraging Splunk for data indexing and machine learning to build a robust health monitoring system. The project focuses on creating an infrastructure that can ingest, process, and visualize health data from various devices. By integrating LangChain AI, I enhanced user interaction through natural language processing and delivered personalized health insights via a mobile app developed with React Native.\n\n Throughout the project, I mastered Splunk's capabilities to develop machine learning models and interactive dashboards, resulting in a functional prototype that offers actionable health insights. This project demonstrates the potential of using Splunk and AI to transform health data into meaningful interventions, contributing to the advancement of personalized medicine and setting the stage for future innovations in healthcare delivery.\n\n The significance of this project is profound, as it has the potential to revolutionize health monitoring by providing individuals with real-time insights into their well-being and empowering them to make informed health decisions. By integrating advanced technologies and machine learning, my work contributes to the advancement of personalized medicine and population health management."   
     }
     ,
     {
        "time": "2:00 PM - 2:15 PM",
        "projectId": "csse-4-200",
        "title": "Analyzing T-Mobile's Lakehouse Migration",
        "studentName": "Geoff Freeman",
        "studentMajor": "CSSE",
        "projectType": "Internship or Job opportunity",
        "facultyAdvisor": "Mr. Mark Kochanski",
        "posterLink": "./posters/csse/freemangeoff_3590358_122650918_CapstonePoster.png",
        "abstract": "In 2021, T-Mobile Finance division’s Data Solutions and Analytics(DSnA) team administered an Azure Data Warehouse (ADW) that integrated data from across T-Mobile for operational and analytical reporting. It powered many business processes, like materials allocation for network site builds, while also giving insight into the business, such as the number of outstanding inventory orders and the status of site acquisition workflows. Unfortunately, a lack of system governance led to frequent system instability. For example, long running analytical reports would block ETLs, creating data latency, or would block execution of operational reports, causing delays in business processes. At peak query times, the high query volumes and limited concurrency would cause long query queues, causing unpredictable query times for even the simplest queries. This led to a number of anti-patterns, most notably teams that would set up their own reporting servers, which they loaded by extracting large volumes of data from the DSnA team’s ADW instance.\n\n To improve system stability while reducing anti-patterns, the next generation of the DSnA warehouse architecture was designed. The business outcomes identified for the new architecture were:\n\n - minimal data latency: should not introduce undue latency between source and consumer\n\n - predictable performance: queries should take about the same amount of time each execution\n\n - economies of scale: increasing system capacity should scale linearly, not exponentially\n\n - data mobility: Ensure it is easy to query data from different systems within the same compute, reducing the need for data duplication \n\n Additionally, the new architecture must provide value incrementally and without requiring business interruptions. DSnA decided upon is called a “Lakehouse” architecture, which uses open source formats that make it possible to interact with data in cloud storage as if the data were stored in database tables. It uses Delta Tables, with Databricks for the ETL engine, and Microsoft Synapse for the presentation layer. Over the next two years, DSnA migrated their workload to this new lakehouse architecture.\n\n This project analyzed DSnA’s adoption of lakehouse architecture, from ideation to implementation. It examined the tradeoffs between proposed architectures, the process for design and acceptance, the roadmap for implementation, and the execution. This retrospective includes lessons learned, successes and failures, and a frank look at the financial implications.\n\n This analysis provides guidance for other teams considering a lakehouse for their data architecture while also demonstrating skills obtained through the CSSE program."   
     }
     ,
     {
        "time": "2:15 PM - 2:30 PM",
        "projectId": "csse-4-215",
        "title": "Teaching Tools Scheduler with Microsoft Outlook Integration",
        "studentName": "Mark Fong",
        "studentMajor": "CSSE",
        "projectType": "Faculty research",
        "facultyAdvisor": "Mr. Mark Kochanski",
        "posterLink": "./posters/csse/fongtszhang_4351551_122705225_Capstone poster.png",
        "abstract": "A web application integrating Microsoft Outlook Calendar enables users to manage their calendar events efficiently and effectively. Leveraging the Microsoft Graph API for authentication and authorization, the application ensures secure access to calendar data and maintains the integrity of personal information. Backend API endpoints, built using Flask, handle various calendar operations, including event retrieval, creation, updates, and deletion.\n\n Authentication and token acquisition are handled by the Microsoft Authentication Library (MSAL) for Python, ensuring a secure and reliable user experience. The React-based frontend provides an intuitive interface for users to interact with their calendar events, allowing easy viewing, creation, and management of appointments and meetings. The react-big-calendar library displays calendar events in a visually appealing manner, facilitating schedule visualization and planning.\n\n The application's modular design and use of industry-standard libraries make it a valuable reference for developers integrating Microsoft Outlook Calendar with web applications. By utilizing the Microsoft Graph API, developers can create applications that leverage Microsoft's ecosystem while providing a secure and intuitive user experience. The application's focus on security and user experience ensures trustworthiness and reliability for users' personal data.\n\n In conclusion, the Microsoft Outlook Integration Teaching Tool Project provides a comprehensive solution to academic scheduling challenges, leveraging development techniques to enhance efficiency and user experience in an educational environment. Not only simplifies the scheduling process but also supports effective communication between instructors and students, ultimately forging a more organized and accessible academic environment."   
     }
     ,
     {
        "time": "2:30 PM - 2:45 PM",
        "projectId": "csse-4-230",
        "title": "Teaching Tools: Canvas Meeting Scheduler",
        "studentName": "Anna Gracheva",
        "studentMajor": "CSSE",
        "projectType": "Faculty research",
        "facultyAdvisor": "Mr. Mark Kochanski",
        "posterLink": "./posters/csse/grachevaanna_4351527_122702885_Gracheva_Poster.jpg",
        "abstract": "The Canvas Teaching Tool project addresses a significant gap in the Canvas Learning Management System, which lacks an integrated and effective scheduling tool for students and instructors. This absence often forces users to rely on external scheduling tools, leading to confusion and inefficiency. The motivation behind this project is to streamline the scheduling process within Canvas, thereby enhancing its functionality and user experience. By integrating the Google Calendar API, this tool not only enables students to book appointments with their instructors directly within Canvas but also ensures that these appointments are automatically synced to both the student’s and instructor’s Google Calendars. This integration prevents scheduling conflicts by checking for overlapping events on both calendars, thereby ensuring a smooth and error-free booking process.\n\n The primary problem this project tackles is the lack of a native scheduling solution within Canvas that can effectively synchronize with external calendars while preventing conflicts. The approach involved a deep integration of the Google Calendar API, allowing for real-time updates to both the student’s and instructor’s calendars. This required a comprehensive understanding of the existing codebase and the implementation of new features for creating, updating, and deleting calendar events. Additionally, the project automated the population of database tables with user and course data from Canvas, eliminating the need for manual data entry.\n\n The results of this project are substantial, providing a fully functional scheduling tool within Canvas that integrates seamlessly with Google Calendar. This integration not only simplifies the scheduling process but also ensures that users are aware of potential conflicts before appointments are made, thus improving the overall efficiency of the system. The successful implementation of this tool demonstrates the potential for further enhancements within Canvas and serves as a foundation for future developments.\n\n In conclusion, the Canvas Teaching Tool project significantly improves the usability of Canvas by incorporating essential scheduling features that were previously missing. This project highlights the importance of understanding and integrating third-party APIs with existing systems to enhance functionality and user experience."   
     }
     ,
     {
        "time": "2:45 PM - 3:00 PM",
        "projectId": "csse-4-245",
        "title": "Test Automation Engineer @ intel VIA Cinder Staffing",
        "studentName": "Chandra Pandian ",
        "studentMajor": "CSSE",
        "projectType": "Internship or Job opportunity",
        "facultyAdvisor": "Mr. Mark Kochanski",
        "posterLink": "./posters/csse/pandianchandra_4288001_122744922_capstone poster final.png",
        "abstract": "For my computer science capstone, I chose to document my work as a full-time test automation engineer at Intel in Hillsboro, OR (contracted through Cinder Staffing). My primary responsibilities include automating software regression tests using primarily C# and C++, and maintaining automation infrastructure. I am part of the validation team for Intel’s “Innovation Platform Framework” (IPF), which is a set of software drivers and SDK’s enabling third party developers to best utilize hardware on an Intel-based platform.\n\n I started this role in February of this year, so by the start of the capstone in June, I had introductory experience with automation and product features. Much of my summer was spent on triage, which involves regular monitoring of failures reported by the automated test suite. Such failures can be either of the product itself (which must be reported as bugs for fixes) or automation infrastructure. My remaining time was spent automating new tests and fixing broken infrastructure.\n\n A notable experience during my capstone was adding automation support for testing a product feature previously only possible with manual testing. This involved requirements analysis in order to replicate the use case of the tested feature, and collaboration with fellow engineers in order to share knowledge and contribute to components of the final solution. As a result, our validation team was able to automate numerous tests, thereby increasing testing coverage of IPF.\n\n Over the course of the last few months, I was able to contribute meaningfully to my team’s automation efforts, and gain expertise of IPF and its test automation infrastructure; this is what I had set out to do when first outlining my capstone. In addition to collaboration and requirements analysis, I both utilized and gained experience in competencies such as software architecture, quality assurance and contemporary software engineering tools. Going forward, I am confident that these skills will be valuable at this role and the rest of my software engineering career."   
     }
     ,
     {
        "time": "3:00 PM - 3:15 PM",
        "projectId": "csse-4-300",
        "title": "Software Development Engineer Intern",
        "studentName": "Rhys Fernandes",
        "studentMajor": "CSSE",
        "projectType": "Internship or Job opportunity",
        "facultyAdvisor": "Prof. Laurie Anderson",
        "posterLink": "./posters/csse/fernandesrhys_4152279_122742801_Fernandes-Poster-Capstone.png",
        "abstract": "The outcome of my internship period is the expansion of the advanced polling features available for Kinesis and DyanamoDB streams to more destinations to help support the variety of eventing needs. Currently, the only destination available for these event sources was AWS Lambda Functions. Lambda is a serverless compute service offered by Amazon Web Services to run code when triggered by an event without the need to manage any compute resources. The current system allows customers to pull data from a source and run their own business logic on Lambda Functions. This project enables customers to use the advanced polling features currently being offered to Lambda destination to various other destinations.\n\n This expansion of polling features posed a unique challenge for this project, as the system is built on top of Lambda. This required a deep dive and to have a deeper understanding of the current service to design and implement a clean and generic approach to ensure that all capabilities are passed to the new system. However, the new destination logic expected needed to be researched to ensure the data is able to reach them efficiently and effectively without changing the entire system. Using a new technology outside of Lambda and EC2 had its own challenges around connectivity, security, and load balancing configurations. These challenges called for extensive research, the creation of a design document with detailed explanations of high-level designs, and collaboration to ensure a well-defined and thought-out solution. \n\n This project was an exciting step for AWS Serverless portfolio as it addresses the demands of customers for expanding the available advanced polling features to other destinations for processing data streams from event sources. It breaks down current barriers for runtime limits and the need for additional layers of services, which currently adds complexity to customer's use cases. Additionally, the project encouraged the refactoring of the destination invocation logic for the current system to ensure readability, scalability, and adaptability for future enhancements to the poller."   
     }
    ]
}
